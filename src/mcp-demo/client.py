# mcp_client_demo.py
import asyncio
from typing import Optional
from contextlib import AsyncExitStack
from dotenv import load_dotenv
from ollama import AsyncClient
from mcp import ClientSession
from mcp.client.stdio import stdio_client, StdioServerParameters
import os

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class MCPClientDemo:
    def __init__(self):
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.llm_client = AsyncClient()

    async def connect_server(self, server_script: str):
        """è¿æ¥MCPæœåŠ¡å™¨[1,6](@ref)"""
        server_params = StdioServerParameters(command="uv", args=["run", server_script])

        # å»ºç«‹stdioé€šä¿¡é€šé“
        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params)
        )
        self.reader, self.writer = stdio_transport

        # åˆå§‹åŒ–ä¼šè¯
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(self.reader, self.writer)
        )
        await self.session.initialize()

        # è·å–å·¥å…·åˆ—è¡¨
        tools = await self.session.list_tools()
        print(f"ğŸ”§ å¯ç”¨å·¥å…·: {[t.name for t in tools.tools]}")

    async def process_query(self, query: str):
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢[5,8](@ref)"""
        # æ„é€ LLMè¯·æ±‚å‚æ•°
        messages = [{"role": "user", "content": query}]
        tools = await self._prepare_tools()

        # è°ƒç”¨LLMç”Ÿæˆå“åº”
        response = await self.llm_client.chat(
            model="qwen3:14b",
            messages=messages,
            tools=tools,
        )

        # å¤„ç†å·¥å…·è°ƒç”¨
        tool_calls = response.message.tool_calls
        if tool_calls:
            print("ğŸ› ï¸ æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨è¯·æ±‚:")
            for call in tool_calls:
                tool_name = call.function.name
                args = call.function.arguments
                print(f"è°ƒç”¨å·¥å…·: {tool_name} å‚æ•°: {args}")

                # æ‰§è¡Œå·¥å…·è°ƒç”¨
                result = await self.session.call_tool(tool_name, args)
                print(f"ğŸ”§ å·¥å…·æ‰§è¡Œç»“æœ: {result}")

                # å°†ç»“æœåé¦ˆç»™LLM
                messages.append(
                    {
                        "role": "tool",
                        "content": str(result),
                        "tool_call_id": call.get("id"),
                    }
                )

                # è·å–æœ€ç»ˆå“åº”
                final_response = await self.llm_client.chat(
                    model="qwen3:14b", messages=messages, stream=True
                )
                async for chunk in final_response:
                    print(chunk.message.content, flush=True, end="")
        else:
            print("ğŸ“ ç›´æ¥å“åº”:", response.message.content)

    async def _prepare_tools(self):
        """è½¬æ¢å·¥å…·æ ¼å¼[2,6](@ref)"""
        tools_response = await self.session.list_tools()
        return [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema,
                },
            }
            for tool in tools_response.tools
        ]

    async def close(self):
        """å…³é—­è¿æ¥"""
        await self.exit_stack.aclose()


async def main():
    client = MCPClientDemo()
    try:
        # è¿æ¥æœ¬åœ°MCPæœåŠ¡å™¨ï¼ˆéœ€æå‰è¿è¡Œï¼‰
        await client.connect_server("{cwd}/src/mcp-demo/server.py".format(cwd=os.getcwd()))

        # ç¤ºä¾‹æŸ¥è¯¢
        while True:
            query = input("\nâ“ è¾“å…¥æŸ¥è¯¢ï¼ˆè¾“å…¥qé€€å‡ºï¼‰: ")
            if query.lower() == "q":
                break
            await client.process_query(query)
    finally:
        await client.close()


if __name__ == "__main__":
    asyncio.run(main())
